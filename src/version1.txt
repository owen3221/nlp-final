import numpy as np
from gen import get_response_google, get_response_local
from util import custom_extract_answer
from prompt import QUERY_TEMPLATE_MULTICHOICE
import math
from .exp import save_responses
from .util import get_result_dir
from prompt import METHOD2_TEMPLATE_MULTICHOICE, QUERY_TEMPLATE_MULTICHOICE, PRIDE_TEMPLATE

def softmax(logits: np.ndarray) -> np.ndarray:
    ex = np.exp(logits - np.max(logits))
    return ex / ex.sum()

def extract_choice_logprobs(response: dict, choices: list[str]) -> np.ndarray:
    """
    從 Google 回傳的 response 取出對每個選項的 log-probability
    回傳一個長度 n 的 np.array([log p(o1), …, log p(on)])
    實作時可參考 response["candidates"][0]["logprobs"] 或 token-by-token 切分。
    """
    try:
        top_lp0 = response["candidates"][0]["logprobs"]["top_logprobs"][0]
    except (KeyError, IndexError):
        # 如果沒有 top_logprobs，就退回到 avg_logprob 的相對值
        avg = response.get("avg_logprob", 0.0)
        return np.array([avg] * len(choices), dtype=float)

    # 構造 log‐prob 列表
    logps = []
    for i in range(len(choices)):
        # 期望字母 "A", "B", "C", "D"
        letter = chr(ord("A") + i)
        # 在 top_lp0 裡尋找匹配的 token
        lp = None
        for tok, v in top_lp0.items():
            if tok.strip().upper() == letter:
                lp = v
                break
        # 若未找到，就给一个很小的 log‐prob (接近0)
        if lp is None:
            lp = math.log(1e-8)
        logps.append(lp)

    return np.array(logps, dtype=float)
    ...

def estimate_prior(D_e: list[tuple], model: str) -> np.ndarray:
    """
    階段 1:用 K=|D_e| 個樣本做循環置換，計算每個選項 ID 的先驗 log-prob 平均,softmax 後回傳全局 prior。
    D_e 裡每筆是 (question, choices, answer)
    """
    all_priors = []
    for question, choices, answer in D_e:
        n = len(choices)
        # 存放對每個置換的 log-probs
        permuted_logits = []
        for shift in range(n):
            # cycle-permute choices
            permuted = choices[-shift:] + choices[:-shift]
            prompt = QUERY_TEMPLATE_MULTICHOICE.format(
                Question=question, A=permuted[0], B=permuted[1],
                C=permuted[2], D=permuted[3]
            )
            rsp = (model=="google"
                   and get_response_google([prompt])[0]
                   or get_response_local([prompt])[0])
            logps = extract_choice_logprobs(rsp, choices)
            permuted_logits.append(logps)
        # 對 n 次置換取平均 log-prob，再 softmax
        avg_log = np.mean(permuted_logits, axis=0)
        all_priors.append(softmax(avg_log))
    # 全局 prior：把所有樣本的 prior 平均
    return np.mean(np.stack(all_priors), axis=0)

def debias_and_predict(question: str, choices: list[str],
                       model: str, prior: np.ndarray) -> int:
    """
    階段 2:對單一樣本，用 default order 呼叫一次，提取 log-probs,
    用 observed_logps - log(prior) 做 debias,再 softmax,取 argmax。
    回傳最後預測的選項 index (0~3)。
    """
    prompt = QUERY_TEMPLATE_MULTICHOICE.format(
        Question=question, A=choices[0], B=choices[1],
        C=choices[2], D=choices[3]
    )
    rsp = (model=="google"
           and get_response_google([prompt])[0]
           or get_response_local([prompt])[0])
    observed = extract_choice_logprobs(rsp, choices)
    debiased_logits = observed - np.log(prior)
    probs = softmax(debiased_logits)
    return int(np.argmax(probs))

def create_pride_exp(dataset, lang: str, model: str, K: int = 10):
    """
    對整個實驗集合做 PriDe
    1) 從 dataset 抽前 K 筆作為 D_e,estimate_prior
    2) 剩下 Dr,對每筆呼 debias_and_predict,存檔
    """
    # 1. 拆分 D_e 與 Dr
    samples = [(d["question"], d["choices"], d["answer"]) for d in dataset]
    D_e, D_r = samples[:K], samples[K:]
    # 2. 估計先驗
    prior = estimate_prior(D_e, model)
    # 3. 對 Dr 做高效 debias
    from .util import get_result_dir, save_responses  # 如有 save_responses
    for idx, (q, choices, ans) in enumerate(D_r):
        pred_idx = debias_and_predict(q, choices, model, prior)
        # 用專案既有的 save_responses 來存 single JSONL
        # 4) 準備 prompt 與 response dict
        prompt = PRIDE_TEMPLATE.format(
            Question=q, A=choices[0], B=choices[1],
            C=choices[2], D=choices[3]
        )
        # （可選）把原始 API 回包也存進去：
        raw = get_response_google([prompt])[0] if model=="google" else get_response_local([prompt])[0]
        responses = [{
            "request": prompt,
            "raw_response": raw,
            "pred_idx": pred_idx,
            "final_probs": (
                lambda o, p: p.tolist()
            )(choices, softmax(observed - np.log(prior))),
        }]

        # 5) 正式呼 save_responses
        save_responses(
            model=model,
            prompt_method="pride",
            requests=[prompt],
            responses=responses,
            lang=lang,
            subject=subject,
            answer=ans,
            index=idx,
        )
"""Evaluate the model with simple_evals/mmlu_eval.py."""

import json
import math
import sys
from pathlib import Path
from pprint import pprint

import numpy as np
import pandas as pd

sys.path.append("./src")
from exp import get_result_dir
from mmmlu_metadata import nlp_final_languages, subtasks, subtasks_for_taskc
from util import custom_extract_answer


def get_acc(path: str, model="google"):
    """
    Get the score from the jsonl file.
    """
    with Path(path).open("r", encoding="utf-8") as f:
        lines = f.readlines()
    responses = []
    for line in lines:
        response = json.loads(line)
        responses.append(response)

    scores = []
    for i, response in enumerate(responses):
        extracted_answer = custom_extract_answer(response, model=model)
        score = 1.0 if extracted_answer == chr(i + ord("A")) else 0.0
        scores.append(score)
    return {
        "mean": sum(scores) / len(scores),
        "unshuffled": scores[responses[0]["answer"]],
        "first": scores[0],
        "second": scores[1],
        "third": scores[2],
        "last": scores[-1],
        "max": max(scores),
        "min": min(scores),
    }


def get_acc_from_result_dir(path: str, model="google"):
    """
    Get the scores from all jsonl files in the directory.
    """
    scores = {
        "mean": [],
        "unshuffled": [],
        "first": [],
        "second": [],
        "third": [],
        "last": [],
        "max": [],
        "min": [],
    }
    for file in Path(path).rglob("*.jsonl"):
        score = get_acc(file, model=model)
        scores["mean"].append(score["mean"])
        scores["unshuffled"].append(score["unshuffled"])
        scores["first"].append(score["first"])
        scores["second"].append(score["second"])
        scores["third"].append(score["third"])
        scores["last"].append(score["last"])
        scores["max"].append(score["max"])
        scores["min"].append(score["min"])

    scores["mean"] = sum(scores["mean"]) / len(scores["mean"])
    scores["unshuffled"] = sum(scores["unshuffled"]) / len(scores["unshuffled"])
    scores["first"] = sum(scores["first"]) / len(scores["first"])
    scores["second"] = sum(scores["second"]) / len(scores["second"])
    scores["third"] = sum(scores["third"]) / len(scores["third"])
    scores["last"] = sum(scores["last"]) / len(scores["last"])
    scores["max"] = sum(scores["max"]) / len(scores["max"])
    scores["min"] = sum(scores["min"]) / len(scores["min"])
    scores["min-max"] = scores["max"] - scores["min"]
    # std among first, second, third, last
    scores["rstd"] = float(
        np.std([scores["first"], scores["second"], scores["third"], scores["last"]])
    )
    # plot first, second, third, last
    import matplotlib.pyplot as plt

    x = ["A", "B", "C", "D"]
    y = [scores["first"], scores["second"], scores["third"], scores["last"]]
    plt.bar(x, y)
    plt.xlabel("Gold Answer")
    plt.ylabel("Accuracy")
    plt.title("Accuracy of when all answers are placed to certain positions")
    plt.savefig(f"{path}/acc.png")
    plt.clf()

    return scores


def rsd(y_true, y_pred):
    """From RSD."""
    classwise_acc = [
        (y_pred[y_true == label] == label).mean() for label in np.unique(y_true)
    ]
    acc = (y_pred == y_true).mean()
    if acc == 0.0:
        return 0.0
    return np.std(classwise_acc) / acc


def get_rsd_from_result_dir(path: str, model="google"):
    """
    Get the scores from all jsonl files in the directory.
    Give choices that are not the answer zero prob.
    """
    responses = []
    for file in Path(path).rglob("*.jsonl"):
        with Path(file).open("r", encoding="utf-8") as f:
            lines = f.readlines()
        for line in lines:
            response = json.loads(line)
            responses.append(response)

    y_true = []
    y_pred = []
    y_probs = []
    for response in responses:
        extracted_answer = custom_extract_answer(response, model=model)
        y_true.append(chr(response["answer"] + ord("A")))
        y_pred.append(extracted_answer)
        answer_prob = math.exp(response["candidates"][0]["avg_logprobs"])
        prob = [0 for i in range(4)]
        prob[response["answer"]] = answer_prob
        y_probs.append(prob)

    return rsd(np.array(y_true), np.array(y_pred))


def fluctuation_rate(
    ans1: list[str],
    ans2: list[str],
):
    """
    Calculate the fluctuation rate between two lists of answers.
    """
    # TODO: Implement the FR metric.
    pass


def get_fr_from_result_dir(path: str):
    """
    Get the fluctuation rate from all jsonl files in the directory.
    """
    # TODO: Gather all FR in one subtask.
    pass


def gen_common_df(
    langs,
    subjects,
    model,
    prompt_method,
):
    """
    Generate a common dataframe for all languages and subjects.
    """
    for metric in ["mean", "rstd", "min-max"]:
        print(f"Metric: {metric}")
        scores = {lang: {} for lang in langs}

        for lang in langs:
            for subject in subjects:
                path = get_result_dir(
                    lang=lang,
                    subject=subject,
                    model=model,
                    prompt_method=prompt_method,
                )
                if not Path(path).exists():
                    continue
                acc = get_acc_from_result_dir(path, model=model)
                scores[lang][subject] = acc[metric]

        scores_df = pd.DataFrame(scores)
        print(scores_df)
        # plot heatmap
        import matplotlib.pyplot as plt
        import seaborn as sns

        plt.figure(figsize=(10, 8))
        sns.heatmap(scores_df, annot=True, fmt=".2f", cmap="Blues")
        plt.title(f"Heatmap of {metric} scores")
        plt.savefig(f"pngs/{model}_{prompt_method}_{metric}_heatmap.png")


# gen_common_df(
#     langs=[obj[0] for obj in list(nlp_final_languages.values())],
#     subjects=subtasks,
#     model="local",
#     prompt_method="multichoice",
# )

# gen_common_df(
#     langs=[obj[0] for obj in list(nlp_final_languages.values())],
#     subjects=subtasks,
#     model="local",
#     prompt_method="cot",
# )


# gen_common_df(
#     langs=["EN_US", "JA_JP"],
#     subjects=subtasks_for_taskc.values(),
#     model="local",
#     prompt_method="cot",
# )

# gen_common_df(
#     langs=["EN_US", "JA_JP"],
#     subjects=subtasks_for_taskc.values(),
#     model="local",
#     prompt_method="multichoice",
# )

gen_common_df(
    langs=["EN_US", "JA_JP"],
    subjects=subtasks_for_taskc.values(),
    model="google",         
    prompt_method="pride",
)
